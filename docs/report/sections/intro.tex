% !TeX spellcheck = en_GB
\documentclass[../main.tex]{subfiles}


\begin{document}

\chapter{Introduction}
\cite{SolvingNature}
-- Problem challenge: Large state space, single goal state 

-- Do without domain knowledge

-- Learn to solve planning problems

-- Rooted in group theory: application of machine learning methods to mathematics

\cite{RubiksMedium}
-- Example of use of reinforcement learning in combinatorial optimization which also includes problems such as travelling salesman, protein folding simulation, resource allocation

\section{State of the Art}
\cite{RubiksMedium}

-- Algorithmic ways to solve the cube can be divided into group theory algorithms such as Kociemba's algorithm and brute force algorithms combined with crafted heuristics

--



\cite{SolvingNature}
-- Puzzle specific pattern databases solves these rubiks well


-- DeepCubeA solves all test configurations and generalizes to other puzzles

-- A shortest path (non-learned) solver is iterative deepening A* search with heuristic from puzzle specific pattern database and uses large data base and group theory 

\subsection{DeepCubeA}
\cite{SolvingNature}
-- Combines DL with approximate value iteration and batch weighted A* search

-- DNN approximates cost-to-go and is called deep approximate value iteration. It only looks one step ahead but multi-step lookahead  and MC tree search was not found to be better

-- DeepCubeA builds on another implementation which was based on policy and Monte Carlo tree search 

-- Learned cost-to-go function then used as heuristic to find path to goal state using weighted A* search

-- DNN: Two fully connected hidden layers, four residual blocks, linear output unit
%Scrambling between 100 and 10000 times + included furthest states.
\end{document}