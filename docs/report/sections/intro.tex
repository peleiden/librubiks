% !TeX spellcheck = da_DK
\documentclass[../main.tex]{subfiles}


\begin{document}
\lhead{Introduktion}
\chapter{Introduktion}
\section{Problemet og dets perspektiv}
\subsection*{Reinforcement Learning og diskret optimering}
%TODO: Tilføj henvisninger til nogle af påstandene
Reinforcement Learning er en af de mest generelle læringsparadigmer i maskinlæring: 
Den enkle opsætning med en agent, et miljø og maksimering af kumulativ skalar-belønning indfører få antagelser om problemets natur og gør sammenligninger til menneskelig læring og fantasier om kunstig generel intelligens fristende.
Når de helt store fremtidsperspektiver lægges til side, er der ofte praktiske mål som selvkørende biler, produktionsrobotter og andre autonome systemer, som bliver set som Reinforcement Learnings potentiale.
Sådanne fysiske agenter i kontinuerte, dynamiske miljøer falder oplagt ind, når man tænker på Reinforcement Learnings formulering og dets familieforhold til kontrolteori, der har rig tradition for styring af fysiske systemer. 

Det er dog unødvendigt begrænsende at se dette lovende læringssystem som dybt forbundet til det kontinuerte og fysiske: Mængden af praktiske beslutningsproblemer er  alsidig og der findes vigtige opgaver med helt andre modelleringsudfordringer end bevægelse af et robotlegeme.

Diskret optimering er et dybt og velstuderet felt indenfor anvendt matematik og datalogi med klassiske problemer som \textit{Traveling Salesman}, grundlæggende datastrukturer som grafer og matroider og vigtige ingeniørpraktiske anvendelser som operationsanalyse, computationel kemi og planlægningssystemer. 
Disse problemer, der kan udmynte sig i kombinatorisk optimering vha. heltalsprogrammering eller optimering under bibetingelser, har ofte dybe, matematiske strukturer beskrevet af abstrakt algebra, herunder gruppeteori.
 
I denne rapport vil Reinforcement Learnings generaliserbarhed undersøges ved at bruge det som løsningsmetode til et sådant  diskret problem, der konventionelt løses med algoritmer fra gruppeteorien.
Det større mål er altså at opnå erfaringer ved at bruge Reinforcement Learning på det kombinatoriske beslutningsproblem, perspektivere det til domænespecifikke løsningsstragier og undersøge udfordringerne med Reinforcement Learning i et stort udfaldsrum. 

\subsection*{Rubiks Terning}

Rubiks terning er et lille, kombinatorisk legetøj opfundet af den ungarske billedhugger og professor i arkitektur Ernö Rubik i 1974. Med 350 mio. solgte terninger regnes den seksfarvede terning som det bedst sælgende legetøj og har fortsat en dedikeret fanskare organiseret i \textit{World Cube Association}, der afholder \textit{speedcuber}-arrangementer og deler praktiske løsningsalgoritmer \cite{RubiksWiki}. 
\begin{figure}[H]
	\centering 
	\includegraphics[width=.5\linewidth]{wiki_rubiks_colors}
	\caption{Illustration of de seks farvede sider i en 3x3 Rubiks terning.\protect\footnotemark}\label{fig:rubcolours}
\end{figure}
\footnotetext{Billede: Wikimedia Commons på \url{https://en.wikipedia.org/wiki/File:Rubik\%27s_cube_colors.svg}}
\noindent Målet er ved en række enkle rotationstræk at få alle siderne til at være ens som på figur \ref{fig:rubcolours}. 
Har man selv sat sig ned med en sådan terning, kan det opleves, at problemet virker enkelt til at starte med, men hurtigt fremstår overraskende svært, når hver handling påvirker flere sider på forskellige måder. Og uden en præcis løsningsvejledning, kan det forekomme umuligt at nå frem en helt udrullet terning.

Dette skyldes, at der kun er én måltilstand og et utroligt stort tilstandsrum (mere om det i kapitel \ref{chp:data}).
Uden en algoritme vil det formentlig tage et gennemsnitligt menneske utroligt lang tid at finde en måde at løse Rubiks terning på, når så stort et rum skal udforskes -- det tog Rubik selv en måned at finde en 
løsningsalgoritme til problemet. 
Det er denne udforskning af udfaldsrummet, der bliver læringsagentens store opgave. 



\section{Nyeste litteratur}

Siden Ernö Rubik i 1974 opfandt sin verdensberømte terning, er der blevet udledt adskillige løsningsalgoritmer (den første af Rubik selv omkring en måned efter terningens opfindelse \cite{ErnoRubik}). 
Overordnet set kan disse inddeles i to typer; 
algoritmer funderet i gruppeteori (såsom Kociembas algoritme \cite{Kociemba}, der anses som værende én af de mest effektive algoritmer til at løse Rubiks terning) og brute force-algoritmer, der løser terningen ved hjælp af bestemte heuristikker til de specifikke tilstande af terningen.  
Brute force-metoderne er oftest relativt nemme for mennesker at lære, men meget ineffektive. 
Andre algoritmer er optimerede til at minimere hukommelses- og/eller tidsforbruget for en computer.  

En nyere tilgang til løsning af Rubiks terning (og andre diskrete problemer generelt) vha. computerkraft er dyb reinforcement learning. 
De største fremskridt inde for netop dette problem blev gjort af et forskerhold fra University of California Irvine, først i \textit{Solving the Rubik's Cube Without Human Knowledge} (maj 2018) \cite{HumansBeGone}, og senere \textit{Solving the Rubik’s cube with deep reinforcement learning and search} (juli 2019) \cite{SolvingNature}.
Det er disse to artikler, som denne rapport tager udgangspunkt i under redegørelsen af den nyeste litteratur inde for området. 

\cite{HumansBeGone} introducerer agenten DeepCube, som bruger autodidaktisk iteration (ADI) til at løse Rubiks terning. 
ADI er en approksimativ værdiiterations-algoritme, der bruges til at træne et dybt neuralt netværk, som derefter anvendes sammen med Monte Carlo-træafsøgning (MCTS) for at løse enhver given tilstand af Rubiks terning. 
Denne metode formår at løse samtlige tilfældige testtilstande med en medianløsningslængde på 30 træk, hvilket er mindre end eller svarende til løsningsalgoritmer, der er baserede på domæneviden.

En viderebygning på denne agent er DeepCubeA \cite{SolvingNature}, som er ændret i værdiiterationen og i stedet for MCTS anvender batch-vægtet A*-afsøgning (BWAS) på netværket. Disse ændringer forstærkede agenten nok til at kunne løse 100\% af testkonfigurationerne og tilmed med færrest antal træk 60\% af gangene. Envidere kunne DeepCubeA generaliseres til andre diskrete spil såsom Sokoban. 



\section{Rapportens mål}
Motiveret af Rubiks terning som et miljø for eksperimentation i Reinforcement Learning på svære, kombinatoriske problemer, er det denne rapports mål at udforske mulighederne og begrænsningerne ved at bruge Reinforcement Learning på at løse Rubiks terning.
Der ønskes altså at skabes en løsningsalgoritme, som ikke bruger problem-specifik viden og derfor selv skal lære problemet at kende og udvikle strategier gennem et Reinforcement Learning-paradigme. 
Den generelle form til agenten er på baggrund af litteratursammenligningen valgt til at være en grafsøgende agent, der bruger lærte, dybe neurale netværk som heuristik til at afsøge tilstandsrummet frem til løsningen. For at tage udgangspunkt i en forholdsvis direkte løsningsmetode, er det et hovedmål for rapporten at genskabe metoderne (men ikke nødvendigvis resultaterne) for agenten \textit{DeepCube} fra \cite{HumansBeGone}.

Der vil derfor implementeres et dybt neuralt neværk, der både approksimerer en værdi til hver tilstand og den optimale politik hertil. Dette netværk trænes ved brug af en værdiapproksimativ metode kaldet audodidaktisk iteration, der bruger en tidligere version af netværket samt en breddeførst søgning på dybde 1 til at approksimere værdi og politik til hver tilstand.
Når netværket er færdigtrænet, bruges dette til at gennemføre Monte Carlo træsøgning frem til den færdige tilstand. 

I implementationen af en sådan agent er der en del store udfordringer for konvergens som skyldes problemets kombinatoriske eksplosion, der beskrives i \ref{sec:grouptheory}, hvilket gør det til et fokus for rapporten at optimere forudsætninger for læring og sammenligne detaljer i læringsparadigmer.


Konkret er målet med rapporten er at svare på følgende spørgsmål:
\begin{itemize}
	\item Hvor god kan en Reinforcement Learning-algoritme blive til at løse Rubiks terning uden at have adgang til menneskelige heuristikker? Med \textit{god} menes antallet af træk væk fra den løste tilstand, som algoritmen fast kan løse.
	\item Hvilke forhold er optimale i forhold til en stabil konvergens af værdifunktionen, det neurale netværk, i autodidaktisk iteration? Her tænkes på datarepræsentationen og design af træningsproceduren.
	\item Hvor tidseffektiv kan løsning og træning af RL i et problem med så stort udfaldsrum være? Her er både fokus på  at lære den bedst mulige værdi-approkismation, så løsningstid minimeres, men også på at softwaredesign i træningsfasen gør det muligt for agenten at udforske så meget som muligt af udfaldsrummet indenfor overskuelig tid.
\end{itemize}
\noindent
I følgende kapitel \eqref{chp:data} vil datagrundlaget for opgaven blive beskrevet ved først at motivere problemets sværhedsgrad, indføre terminologi og notation samt vise tilstandsrummets størrelse ved brug af den gruppeteoretiske beskrivelse af terningen. Derefter vil den brugte datastruktur for terningen indføres og afbalancering mellem computationel effektivitet og semantisk passende repræsentation diskuteres.

I kapitel \ref{chp:methods} vil metoderne for løsningen introduceres. Terningemiljøet gennemgås efterfulgt af en introduktion af Reinforcement Learning-opsætningen for problemet. Så introduceres værdi- og politiknetværket samt træningsproceduren, autodidaktisk iteration, samt kostfunktion og optimering for dette. Beskrivelse af løsningsagenten vil så afsluttes med introduktion af Monte Carlo-træsøgningsalgoritmen. Afslutningsvist  vil de gennemførte eksperimenter blive ridset op.



 
\end{document}






















