\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Metode}
\lhead{Metode}
De undersøgt dybe Reinformcent Learning-modeller basererde sig alle på modellen DeepCube fra \cite{HumansBeGone} med nogle undtagelser, der beskrives i de følgende afsnit. Løsningsmodellen består af et neuralt netværk, der trænes med en Reinforcement Learning-procedure kaldet autodidaktisk iteration til at vurdere den værdien af og den optimale handling til enhver tilstand for Rubiks terning. Dette netværk bliver så brugt som heuristisk for en Monte Carlo træsøgning som løsningsstrategi til terningen.

For at undersøge, hvad der er vigtigt for at få gode resultater, udføres der sammenlignende eksperimenterer, hvor der ændres på detaljer i trænignsparadigmet, det neurale netværks struktur samt parametre for træsøgningsmodellen.

\section{Reinforcement Learning: Træning af DNN}

\begin{itemize}
	\item Kort intro af RL-opsætning og reward
	\item Forskel på politik og værdi; her bruges begge dele
	\item Det neurale netværks struktur
\end{itemize}

\subsection{Træningsdata: Autodidaktisk Iteration}


\begin{itemize}
	\item Gennemgang af algoritmen
\end{itemize}

\subsection{Optimering af model}




\section{Løsningsalgoritme og Monte Carlo Træsøgning}

\section{Eksperimenter}




\end{document}