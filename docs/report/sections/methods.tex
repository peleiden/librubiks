\documentclass[../main.tex]{subfiles}

\begin{document}
\chapter{Metode}\label{chp:methods}
\lhead{Metode}
\textit{Al kode er tilgængelig på} \url{https://github.com/sorenmulli/rl-rubiks}
\\
De implementerede dybe Reinformcent Learning-modeller  er alle implementationer af DeepCube fra \cite{HumansBeGone} med nogle undtagelser, der beskrives i de følgende afsnit.
Løsningsmodellen består af et neuralt netværk, der trænes med en Reinforcement Learning-procedure kaldet autodidaktisk iteration til at vurdere værdien af og den optimale handling til enhver tilstand for Rubiks terning.
Dette netværk bliver så brugt som heuristisk for en Monte Carlo træsøgning som løsningsstrategi til terningen.

For at undersøge, hvad der er vigtigt for at få gode resultater, udføres der sammenlignende lærings-eksperimenterer og evalueringseksperimenter, hvor der ændres på detaljer i træningsparadigmet og tiden for træsøgningsmodellen. 

\section{Rubiks terning som miljø}\label{sec:environment}
En af de definerende karaktestikker ved reinforcement learning er agentens og miljøets interaktion.
Det er derfor nødvendigt at have en velfungerende repræsentation af miljøet, på hvilken agenten kan interagere.
Miljøet implementeres i \texttt{python} som en klasse, hvis centrale egenskab er rubiksterningens tilstand som beskrevet i afsnit \ref{sec:repr}.
Miljøet initialiseres med den løste tilstand, hvorefter indbyggede metoder bruges til at foretage givne eller tilfældige rotationer.
Derudover indeholder miljøklassen en metode til at teste for, om den pågældende tilstand er løst, samt en metode, der giver tilstanden i et mere læsevenligt format.

I designet af miljøklassen har der været fokus på optimering under forventningen om adskillige millioner, hvis ikke milliarder, simuleringer under træningen af agenten.
Især rotationsmetoden har været tildelt særligt fokus, da rotationer udgør al interaktion med miljøet, hvorfor det er favorabelt, at de kan foretages hurtigt.

Med miljøet fastlagt er det nu muligt at påbegynde træningen af en agent, der kan interagere intelligent med miljøet og nærme sig eller finde den løste tilstand.


\section{Den lærende agent}\label{sec:learnADI}
Opgaven med at udvikle løsningsmodellen er et veldefineret læringsproblem, da det formaliseres som at følge Reinforcement Learning-opsætningen ved at have følgende aspekter:
\begin{itemize}
	\item \textit{Agent}:
	Løsningsmodellen -- som her kaldes DeepCube, selvom den også afprøves med ændringer i forhold til DeepCube i \cite{HumansBeGone} -- har rollen som lærende agent, der interagerer med miljøet.
	Den består af det dybe neurale netværk (DNN) samt træafsøgningen og beskrives i løbet af dette kapitel.
	Agenten er model-baseret, da den har en indre repræsentation af Rubiks terning for at træ-afsøge tilstande.\\
	Agenten er implementeret i  \texttt{src/rubiks/post\_train/agents.py}.
	\item \textit{Miljø}:
	Rubiks terning, der modtaget handlinger \(a\) og returnerer tilstand \(\mathbf s\).
	Miljøet er en fuldt-observerbar, deterministisk Markovbeslutningsproces, da al information om ny tilstand \(\mathbf s_{k+1}\) kun er afhængig af foregående tilstand og foregående handling \(a_k\):
	\[
		\mathbf s _ {k+1} = \mathbf f(\mathbf s _k, a_k)
	\] 
	og da agentens observation indeholder al information om tilstanden.
	Miljøet har én afsluttende tilstand \(\mathbf s_T\).
	Miljøet er  beskrevet i afsnit \ref{sec:environment} og implementeret i \texttt{src/rubiks/cube.py}.
	\item  \textit{Handlinger}:
	Handlingsrummet \(A\) bestående af de tolv handlinger beskrevet i afsnit \ref{sec:environment} er uafhængigt af tilstanden og tidsskridtet.
	\item \textit{Belønning}:
	Belønningsfunktionen \( R(\mathbf s_k)\) er kun afhængig af tilstanden \(\mathbf s\) og returnerer skalarbelønningen på følgende måde:
	\[
		R (\mathbf s_k) = 
		\begin{cases}
			1  &: \mathbf s_k = \mathbf{s_T}\\
			-1 &: \mathbf s_k \neq \mathbf{s_T}
		\end{cases}
	\] 
	Belønningen er således valgt for at være i overenstemmelse med \cite{HumansBeGone} og for at give minimal information om problemet til agenten, der så testes uafhængigt af menneskeudviklet heuristik.
\end{itemize}
\paragraph{Politik og værdi} Målet med træning af Reinforcement Learning-agenten DeepCube er så godt som muligt at approksimere den optimale politik \(\pi^*\), der maksimerer akkumuleret belønning \(v\), til hvilken der opnås den optimale akkumulerede belønning \(v^*\):
\begin{equation}\label{eq:maximization}
	\pi^* = \argmaxi_{\pi}\sum_{k=0}^{N-1} R\big(f(\mathbf{s_k},\pi(\mathbf s_k))\big),
	v^* = \maxi_\pi\sum_{k=0}^{N-1} R\big(f(\mathbf{s_k},\pi(\mathbf s_k))\big)
\end{equation}
hvor \(	N = \operatorname{min}\{k_T, k_{\text{max}}-1\}\), \(k_T\) er tidsskridtet, når agenten løser terningen og \(k_{\text{max}}\) er en grænse på antal skridt, som defineres i eksperimentet. 

Til at bestemme \(\widetilde {\pi ^*} \approx \pi^*\) blev to sammenhængende afbildninger implementeret i DeepCube.
Den ene komponent er en lært afbildning, der direkte approksimerer \(\widetilde {\pi ^*}\) ved at afbillede fra tilstanden \(\mathbf s_k\) til en diskret sandsynlighedsfordeling, der approksimativt tilskriver hver mulig handling en sandsynlighed for at denne er den optimale.
Denne funktion kaldes \textit{politik-netværket} og repræsenteres fremover med symbolet \(\bm \pi\):
\[
\pi(\mathbf s_k)_i \approx \mathbb P (\pi^*(\mathbf s_k)_i = i ),\quad i \in \{0,1, ..., 11\}
\]
Desuden læres en funktion, der approksimerer \textit{værdien} til hver tilstand forstået som den akkumulerede belønning der fås ved at følge politikken \(\pi\) herfra.
Denne kaldes \textit{værdi-netværket} og repræsenteres som \(v\):
\[
v(\mathbf s_k| \pi) \approx \sum_{j=k}^{N-1} R\big(f(\mathbf s_j,\pi(\mathbf s_j))\big)
\]
\paragraph{Dybt neuralt netværk} Disse to afbildninger, \(\bm \pi, v\), læres altså i løbet af træningsprocessen og har centrale roller som dybde- og breddestyrende heuristikker i  løsningsalgoritmen.
For at repræsentere disse approksimationer blev et dybt neuralt netværk (DNN) brugt.
Da disse afbildninger begge skal approksimere værdier relateret til  \eqref{eq:maximization}, blev de udtrykt i det samme neurale netværk, der defor blev konstrueret med en todelt outputstruktur. 


\begin{figure}[H]
	\centering
\input{imgs/NNtikz.tex}
\caption{Illustration af det todelte neurale netværk brugt i \textit{DeepCube}, hvor to skjulte lag skal resultere i hierakisk vidensopbygning brugbart for at repræsenterer løsningsstrategier.}
\label{fig:DNN}
\end{figure}
\noindent
Netværket består af lineære lag med bias.
Arkitekturen, som er illustreret på figur \ref{fig:DNN}, består af et 480-dimensionelt inputlag efterfulgt af to fuldt-forbundne skjulte lag på hhv. 4096 og 2048 neuroner.
Derefter opdeles netværket i værdi-netværket og politiknetværket, begge med et skjult lag på 512 neuroner.
Værdinetværket returnerer en skalarværdi \(v(\mathbf s_k)\) og politiknetværket returnerer den tolvdimensionelle vektor \(\bm \pi (\mathbf s_k)\).
Denne struktur medfører ca. 25 millioner parametre i det dybe neurale netværk.
Mellem hvert lag blev den elementvise ikkelineære aktiveringsfunktion \textit{Eksponentiel lineær enhed} (eng: \textit{Exponential Linear Unit}, ELU) placeret, som i nogle læringsproblemer er vist at accelerere læringen bl.a. ved at have en normaliserende effekt på aktiveringerne \cite{ELU} 
\[
\operatorname{ELU}(x) = \maxi\{0, x\} + \operatorname{min}\{0, \eul{x} -1\}
\]
På det tolvdimensionelle politikoutput blev softmax-aktiveringen brugt for at fortolke den som en diskret sandsynlighedsfordeling:
\[
\pi(\mathbf s_k)_i = \sigma(\mathbf z)_i = \frac{\eul {z_i}}{\sum_{j=0}^{11} \eul{z_j}}
\]
hvor \(\mathbf z=(z_0, ..., z_i, ..., z_{11})\) er output fra politiknetværket.
Det neurale netværk er implementeret i \texttt{src/rubiks/model.py}

\subsection*{Læring af værdi og politik: Autodidaktisk iteration}
For at opnå et politik/værdi-netværk med gode approksimationer kræves et eksplorativt læringsparadime hvorigennem netværket opnår nok kendskab til tilstandsrummet \(S\) til at kunne finde generelle strategier. 
På trods af problemets determinisme og Markov-egenskab er der dog en stor udfordring i udforskningen af disse tilstande.
Som beskrevet i afsnit \ref{sec:grouptheory}, gør de mange mulige permutationer, at tilstandsrummet når en størrelse på \(|S|\approx 4,3\ctp {19}\), hvilket gør, at enhver systematisk løsning af maksimeringsproblemet \ref{eq:maximization} kun kan kun kan tilskrive optimal værdi og politik til en forsvindende lille andel af tilstandsrummet.
Der er derfor behov for et læringsprocedure, der kan observere approksimativ optimal politik og værdi til tilstande på kort tid samt vælge at udforske tilstande, der giver et brugbart billede af tilstandsrummet.

Til træningen af politik/værdi-netværket blev en iterativ metode, der tilskriver tilstande værdi ud fra den til hver tid nyeste version af netværket brugt. 
Metoden er præsenteret i \cite[4.1]{HumansBeGone}, hvor den kaldes \textit{Autodidaktisk iteration} (ADI) og tager udgangspunkt i den løste Rubiks terning \(\mathbf s_T\).
For denne terning tages \(K\) tilfældige træk og for hvert træk gemmes tilstanden opnået ved disse træk samt antallet af tilfældige træk hørende til tilstanden \(D(\mathbf s_i)\). 

Der er altså nu skabt \(K\) tilstande \(\mathbf{s}_{i}, i\in\{0..K-1\}\). For hver af disse skal der tilskrives de bedst mulige observationer for tilstandens værdi \(y_v\) og politik \(y_{\bm \pi }\), som det ønskes at politik/værdi-netværket skal approksimere.
For at finde disse, udføres nu alle tolv mulige handlinger til den pågældende tilstand. For hver af disse tolv subtilstande \(f(\mathbf s_i, a), a\in A\), der her fremkommer, evalueres det nyeste \textit{værdi}-netværk og tolv subværdier \(v(a), a\in A\) haves nu for den pågældende tilstand.
Disse tolv subværdier bruges nu til at tilskrive værdi og politik til forældertilstanden \(\mathbf s _i\).	 

Til tilstanden \(\mathbf s _i\) tilskrives nu værdi og politik ved følgende formler:
\begin{align}
	&{y_{v}}_i= \maxi_{a\in A}\  R\big( f(\mathbf s_i, a)\big)+v(a)\label{eq:targetval}
	\\
	& {y_{\bm \pi }}_i = \argmaxi_{a\in A} \  R\big( f(\mathbf s_i, a)\big)+v(a) \label{eq:targetpi}
\end{align}
Der gemmes altså \(K\) observationer af værdi/politik-par til, der skal tilnærme sig den optimale værdi og handling til hver af dem ved at bruge værdinetværket for den følgende tilstand og belønningsfunktionen \(R\). 
Der genereres et større antal observationer ved at spille \(L\) spil, så der opnås \(N=L\cdot K\) træningseksempler.
Træningssættet $ X $ siges at bestå af alle opnåede tilstande $ x_i $, således at $ X=\{x_i\}_{i=1}^N $

Ovenover er autodidaktisk iteration beskrevet, som præsenteret i \cite{HumansBeGone}. En alternativ regel for tilskrivelse af værdi er dog foreslået af Max Lapan \cite{RubiksMedium} for at forbedre stabiliteten af træningen. Denne ændrer formel \eqref{eq:targetval} til at være
\begin{equation}\label{eq:lapantargeval}
y_{v_i}= 
\begin{cases}
0 & \text{hvis \(\mathbf s_i\) er den vindende tilstand}\\
\maxi_{a\in A}\  R\big(\mathbf f(\mathbf s_i, a)\big)+v(a) &\text{ellers}
\end{cases}
\end{equation}
I træningen blev der spillet \(L=10000\) spil i autodidaktisk iteration. Dybden \(K\) blev undersøgt i eksperimenter.

\subsection*{Opdatering af model}
Ovenfor blev en procedure til at generere \(N\) datapunkter, der hver indeholder en tilstand \(\mathbf s_i\), tilsigtede observationer af værdi og politik \(\left({y_{v}}_i, {y_{\bm \pi }}_i \right)\) samt antallet af tilfældige træk taget for at nå frem til tilstanden, betegnet dybden \(D(\mathbf s_i)\).
Disse blev  brugt til at opdatere modellen i et Supervised Learning-paradigme, hvor værdinetværket løser et regressionsproblem med \({y_{v}}_i\) som mål og politiknetværket løser et klassifikationsproblem med \({y_{\bm \pi }}_i\) som mål. 

Som fejlfunktion på netværkets forudsigelse af værdi og politik
\(\left(v(\mathbf s_i), \bm \pi (\mathbf s_i)\right)\)
blev følgende funktion brugt:
\begin{equation}\label{eq:loss}
	\mathcal L(x_i) =
	\underbrace{\vphantom{\sum_{i}}
		v_i
	}
	_{\mathclap{
		\text{Vægt}
		}	
 }
	 \bigg(
		\underbrace{\vphantom{\sum_{i}}
		({x_{v}}_i-{y_{v}}_i )^2
		}
		_{\text{Kvadreret afvigelse}}	
			\underbrace{\vphantom{\sum_{i}}
		- 
		\log 
			\frac{
					\eul{{\bm \pi(\mathbf s_i)}_j
				}
				}
				{
					\sum_{k=0}^{11}
					\eul{{\bm \pi(\mathbf s_i)}_k}
				}
			}
			_\text{Krydsentropi-kost}
	\bigg)
	\text{, hvor } j = {y_{\bm \pi }}_i
\end{equation}
Fejlen til hvert datapunkt består altså af en sum af kvadreret afvigelse for regressionsproblemet på værdien og 12-klasse krydsentropifejl (eng: \textit{Cross Entropy Cost}) for klassifikationsproblemet på politikken.
Summen vægtes med \(v_i\), som er beregnet ud fra antallet af træk fra den løste tilstand, $ D(\mathbf s_i) $.
Vægtningen er indført af \cite{HumansBeGone}, hvor vægten er beregnet ved den inverse dybde, $ v_i=1/D(\mathbf s_i) $.
Det giver konvergensstabilitet ved at vægte tilstande, hvor udfaldsrummet er mindre komplekst.
Her har vi udvidet vægtningsfunktionalitet for at undersøge forskellige vægtningers effekt på læringen.
Vi arbejder med en kombination af den reciprokke dybdevægtning og en vægtning, hvor alle tilstande er lige.
Ved generering af $ n $ træningstilstande dannes to vægtvektorer med længden $ n $, $ \mathbf v^d=[v_1^d, v_2^d, \ldots, v_n^d] $ og $ \mathbf v^u=[v_1^u, v_2^u, \ldots, v_n^u] $ ud fra reglerne $v_i^d = \frac{1}{D(\mathbf s_i)} \andim v_i^u = 1$
Vektorerne normaliseres, og et vægtet gennemsnit beregnes ved hjælp af en vægtfaktor $ alpha $.
Herved samles de i en samlet vægtvektor $ \mathbf v=[v_1, v_2, \ldots, v_n] $.
Normaliseringen sikrer, at der ikke er et uhensigtsmæssigt bias mod den ene af vægtvektorerne.
\begin{equation*}
	\mathbf v=(1-\alpha)\frac{\mathbf v^d}{\sum_{i=1}^n v_i^d} + \alpha \frac{\mathbf v^u}{n}
\end{equation*}
$ \mathbf v $ vil være en enhedsvektor, hvilket giver den fordelagtive egenskab, at summen af alle $ \mathcal L(x_i) $ altid vil være tilnærmelsesvist konstant over $ n $ (se bilag \ref{app:vsum} for et bevis herpå).
Ved $ \alpha=0 $ fås den vægtning, som er indført af \cite{HumansBeGone}, og ved $ \alpha=1 $ fås en ens vægtning for alle tilstande.
Vi initialiserer $ \alpha $ til 0 og øger den derefter med en konstant $ \alpha_u $, hver gang der er trænet i et givent antal udrulninger.
Dog lader vi ikke $ \alpha $ overstige 1.
\\
\\
For at minimere fejlfunktionen blev der brugt minibatchlæring på det dybe neurale netværk med batches på størrelse 50.
Optimeringen foregik med algoritmen RMSProp, der tilpasser læringsraten ved at dividere den med et rullende gennemsnit af gradientmagnituden \cite{RMSProp}.
Til optimeringen blev brugt \texttt{pyTorch} hvis indbyggede \texttt{autograd}-modul foretog denne gradientbaserede parameteropdatering.

Træningen af det dybe neurale netværk foregik ved at udføre autodidaktisk iteration til at generere de \(N\) træningseksempler efterfulgt af træning på dette datasæt. 
En sådan træning kaldes en \textit{udrulning} af netværket, og der blev gennemført 5000 udrulninger.

\section{Løsningsalgoritme og Monte Carlo Træsøgning}
Efter at have trænet det neurale netværk anvendtes Monte Carlo Træsøgning (MCTS) for at kunne løse en terning fra en \emph{scrambled} tilstand $s_0$.
MCTS er en heuristisk søgealgoritme.
Som det ligger i navnet, bygger MCTS et træ af tilstande, hvor roden er $s_0$ og dens børneknuder er de resulterende tilstande efter hver af de 12 mulige handlinger.
MCTS er en effektiv søgealgoritme, fordi den sørger for at ekspandere den mest lovende bladknude først. 

En måde at vælge en knude at ekspandere er ved brug af den øvre konfidensgrænse en øvre konfidensgrænse (\emph{upper confidence bound}, UCB), introduceret af \cite{Kocsis06banditbased}.
For hver bladknude t beregnes UCB
$$UCB(t)=\frac{r_t}{n_t}+\sqrt{\frac{2\ln n_s}{n_t}}$$
og bladknuden med den højeste UCB vælges til at blive ekspanderet.
$s$ er forælderknuden til $t$, og $\frac{r_t}{n_t}$ er belønningen for $t$.
$n_t$ og $n_s$ er hvor mange gange henholdsvis $t$ og $s$ er blevet spillet.
Første led af formlen korresponderer med udnyttelse, da den vil være høj for træk, der leder til den løste tilstand.
Andet led korresponderer med udforskning, da den vil være høj for træk, der kun er blevet simuleret få gange. 

Efter at have valgt en bladknude, genereres en barneknude til denne.
Derefter genereres en række tilfældige træk, der enten resulterer i, at terningen løses, eller at den ikke løses (\emph{rollout}).
Resultatet af \emph{rollout}'en tilbagepropageres ved at opdatere informationen i knuderne på stien fra barneknuden tilbage til $s_0$. 

Der findes også andre, mere specialiserede, politikker for at udvælge en bladknude.
I \cite{HumansBeGone} og \cite{mcaleer2018solving} vælges en handling a til tiden t fra tilstand s ud fra politikken
$$A_t=\text{argmax}_a\frac{cP_{st}(a)\sqrt{\sum_{a'}N_{st}(a')}}{1+N_{st}(a)} + W_{st}(a) - L_{st}(a)$$
hvor $N_s(a)$ er antallet af gange a er blevet taget fra s, $W_s(a)$ er den maksimale værdi af a fra s, $L_s(a)$ er det nuværende virtuelle tab for a fra s, $P_s(a)$ er en prior-sandsynlighed af a fra s og c er en udforsknings-hyperparameter. 
  

\section{Eksperimenter}
Én agent blev trænet og præsenteres som hovedresultatet i rapporten.
Derudover blev en række sammenlignende eksperimenter gennemført for at undersøge nogle af detaljerne i læringsmetoden, netværksarkitektur og løsningsmetoden.

I alle træninger blev der spillet \(N=1000\) spil for hver politik-udrulning. 
Hvert spil blev ekspanderet til dybde \(K=50\) og der blev i alt foretaget \(3000\) udrulninger, inden træningen terminerede.
Læringsraten blev sat til \(1\ctp{-4}\) og blev sænket løbende med \(\gamma = 0,99\).

Alle løsningsevalueringer blev gennemført med en tidsgrænse på 5 sekunder.
Alle \(A^*\)-søgninger blev gennemført med vægtning på \(\lambda=0,2\) og ekspansionsstørrelse på 50.
\paragraph{Hovedagent} 
En agent blev gennemført ved at bruge det fuldt-forbundne DNN på \(20\times24\)-repæsentationen og træne ved at bruge konvergensfixet ved at sætte den løste tilstand til værdi 0.
Den adaptive dybdevægtning blev brugt med \(\alpha=something\). 
Løsningsagenten med \(A^*\)-søgning blev gennemført og evalueret.
\paragraph{Sammenligning af træningsmetoder}
Netværket blev fastholdt til at være det fuldt-forbundne DNN på \(20\times24\)-repæsentationen. 
Det blev derefter trænet på fire forskellige måder
\begin{enumerate}
	\item Træning, som den er beskrevet i \cite{HumansBeGone} hvor der ikke gennemføres konvergensfix, så den løste tilstands værdi ikke fastholdes. 
		Der bruges dybdevægtning.
	\item Træning, som den er beskrevet i \cite{RubiksMedium} hvor konvergensfixet indføres og der bruges dybdevægtning.
	\item Træning, hvor der ikke bruges dybdevægtning, men bruges konvergensfix.
	\item Træning, hvor der bruges adaptiv dybdevægtning med \(\alpha=sumthang\) og konvergensfix. 
\end{enumerate}
For de fire resulterende netværk blev \(A^*\)-løsningsagenten evalueret.

\paragraph{Sammenligning af netværksarkitekturer og terningerepræsentationer}
Træningsmetoden blev fastholdt til at være med konvergensfix og adaptiv dybdevægtning med \(\alpha=sumthang\).
Fem forskellige kombinationer af terningerepræsentation og neuralt netværk blev trænet.
\begin{enumerate}
	\item Fuldt-forbundet DNN på \(20\times24\)-repræsentationen.
	\item Fuldt-forbundet DNN på \(6\times8\times6\)-repræsentation.
	\item Residualt DNN på \(20\times24\)-repræsentationen
	\item Residuelt DNN på \(6\times8\times6\)-repræsentationen.
	\item Konvolutionelt neuralt netværk på \(6\times8\times6\)-repræsentationen.
\end{enumerate}
For disse fem netværk blev \(A^*\)-løsningsagenten evalueret.

\paragraph{Sammenligning af løsningsmetoder}
Det fuldt-forbundne DNN blev trænet med konvergensfixet og adaptiv dybdevægtning med \(alpha=sumdang\). 
Ud fra dette blev fem forskellige løsningsalgoritmer evalueret.
\begin{enumerate}
	\item Simpel bredde-først søgning, der inkluderes som baseline.
	\item Politik-søgning: Direkte at følge politik-outputtet fra DNN.
	\item Grådig søgning: At evaluere værdien for de 12 mulige undertilstande og træffe den handling, der svarer til den højeste værdi heraf.
	\item Monte Carlo-træsøgning: Gennemføre Monte Carlo-træsøgning med \(c=0,6\).
	\item \(A^*\): Gennemføre \(A^*\)-søgning med \(\lambda=0,2\) og ekspansionsstørrelse på 50.
\end{enumerate}
\end{document}
